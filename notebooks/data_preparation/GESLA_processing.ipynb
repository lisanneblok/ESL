{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lb962/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "/Users/lb962/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "/Users/lb962/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "/Users/lb962/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "/Users/lb962/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "/Users/lb962/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "/Users/lb962/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "/Users/lb962/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "/Users/lb962/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Resulting object does not have monotonic global indexes along dimension date_time",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m file_pattern \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_directory, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Open and combine all NetCDF files into a single dataset\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m combined_ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mby_coords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Inspect the dataset (optional)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_ds)\n",
      "File \u001b[0;32m~/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/backends/api.py:1066\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     combined \u001b[38;5;241m=\u001b[39m _nested_combine(\n\u001b[1;32m   1054\u001b[0m         datasets,\n\u001b[1;32m   1055\u001b[0m         concat_dims\u001b[38;5;241m=\u001b[39mconcat_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m combine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby_coords\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;66;03m# Redo ordering from coordinates, ignoring how they were ordered\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;66;03m# previously\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m     combined \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_by_coords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1076\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is an invalid option for the keyword argument\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1077\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ``combine``\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(combine)\n\u001b[1;32m   1078\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/core/combine.py:973\u001b[0m, in \u001b[0;36mcombine_by_coords\u001b[0;34m(data_objects, compat, data_vars, coords, fill_value, join, combine_attrs, datasets)\u001b[0m\n\u001b[1;32m    969\u001b[0m     grouped_by_vars \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mgroupby(sorted_datasets, key\u001b[38;5;241m=\u001b[39mvars_as_keys)\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;66;03m# Perform the multidimensional combine on each group of data variables\u001b[39;00m\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;66;03m# before merging back together\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m     concatenated_grouped_by_data_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_combine_single_variable_hypercube\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets_with_same_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets_with_same_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgrouped_by_vars\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[1;32m    987\u001b[0m     concatenated_grouped_by_data_vars,\n\u001b[1;32m    988\u001b[0m     compat\u001b[38;5;241m=\u001b[39mcompat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    991\u001b[0m     combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    992\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/core/combine.py:974\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    969\u001b[0m     grouped_by_vars \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mgroupby(sorted_datasets, key\u001b[38;5;241m=\u001b[39mvars_as_keys)\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;66;03m# Perform the multidimensional combine on each group of data variables\u001b[39;00m\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;66;03m# before merging back together\u001b[39;00m\n\u001b[1;32m    973\u001b[0m     concatenated_grouped_by_data_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 974\u001b[0m         \u001b[43m_combine_single_variable_hypercube\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets_with_same_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mvars\u001b[39m, datasets_with_same_vars \u001b[38;5;129;01min\u001b[39;00m grouped_by_vars\n\u001b[1;32m    984\u001b[0m     )\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[1;32m    987\u001b[0m     concatenated_grouped_by_data_vars,\n\u001b[1;32m    988\u001b[0m     compat\u001b[38;5;241m=\u001b[39mcompat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    991\u001b[0m     combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    992\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ESL/lib/python3.12/site-packages/xarray/core/combine.py:648\u001b[0m, in \u001b[0;36m_combine_single_variable_hypercube\u001b[0;34m(datasets, fill_value, data_vars, coords, compat, join, combine_attrs)\u001b[0m\n\u001b[1;32m    646\u001b[0m     indexes \u001b[38;5;241m=\u001b[39m concatenated\u001b[38;5;241m.\u001b[39mindexes\u001b[38;5;241m.\u001b[39mget(dim)\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (indexes\u001b[38;5;241m.\u001b[39mis_monotonic_increasing \u001b[38;5;129;01mor\u001b[39;00m indexes\u001b[38;5;241m.\u001b[39mis_monotonic_decreasing):\n\u001b[0;32m--> 648\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResulting object does not have monotonic\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    650\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m global indexes along dimension \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dim)\n\u001b[1;32m    651\u001b[0m         )\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concatenated\n",
      "\u001b[0;31mValueError\u001b[0m: Resulting object does not have monotonic global indexes along dimension date_time"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "output_directory = \"/Users/lb962/Documents/GitHub/ESL/data/processed_GESLA\"\n",
    "\n",
    "# Create a glob pattern to match all .nc files in the directory\n",
    "file_pattern = os.path.join(output_directory, \"*.nc\")\n",
    "\n",
    "# Open and combine all NetCDF files into a single dataset\n",
    "combined_ds = xr.open_mfdataset(file_pattern, combine=\"by_coords\")\n",
    "\n",
    "# Inspect the dataset (optional)\n",
    "print(combined_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
